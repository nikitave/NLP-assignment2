{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIgM6C9HYUhm"
      },
      "source": [
        "# Context-sensitive Spelling Correction\n",
        "\n",
        "The goal of the assignment is to implement context-sensitive spelling correction. The input of the code will be a set of text lines and the output will be the same lines with spelling mistakes fixed.\n",
        "\n",
        "Submit the solution of the assignment to Moodle as a link to your GitHub repository containing this notebook.\n",
        "\n",
        "Useful links:\n",
        "- [Norvig's solution](https://norvig.com/spell-correct.html)\n",
        "- [Norvig's dataset](https://norvig.com/big.txt)\n",
        "- [Ngrams data](https://www.ngrams.info/download_coca.asp)\n",
        "\n",
        "Grading:\n",
        "- 60 points - Implement spelling correction\n",
        "- 20 points - Justify your decisions\n",
        "- 20 points - Evaluate on a test set\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x-vb8yFOGRDF"
      },
      "source": [
        "## Implement context-sensitive spelling correction\n",
        "\n",
        "Your task is to implement context-sensitive spelling corrector using N-gram language model. The idea is to compute conditional probabilities of possible correction options. For example, the phrase \"dking sport\" should be fixed as \"doing sport\" not \"dying sport\", while \"dking species\" -- as \"dying species\".\n",
        "\n",
        "The best way to start is to analyze [Norvig's solution](https://norvig.com/spell-correct.html) and [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf).\n",
        "\n",
        "You may also want to implement:\n",
        "- spell-checking for a concrete language - Russian, Tatar, etc. - any one you know, such that the solution accounts for language specifics,\n",
        "- some recent (or not very recent) paper on this topic,\n",
        "- solution which takes into account keyboard layout and associated misspellings,\n",
        "- efficiency improvement to make the solution faster,\n",
        "- any other idea of yours to improve the Norvig’s solution.\n",
        "\n",
        "IMPORTANT:  \n",
        "Your project should not be a mere code copy-paste from somewhere. You must provide:\n",
        "- Your implementation\n",
        "- Analysis of why the implemented approach is suggested\n",
        "- Improvements of the original approach that you have chosen to implement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MoQeEsZvHvvi"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\"wget\" �� ���� ����७��� ��� ���譥�\n",
            "��������, �ᯮ��塞�� �ணࠬ��� ��� ������ 䠩���.\n"
          ]
        }
      ],
      "source": [
        "!wget https://norvig.com/big.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to\n",
            "[nltk_data]     C:\\Users\\Nikita\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from collections import Counter\n",
        "\n",
        "import nltk\n",
        "import string\n",
        "import random as rd\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "bigrams = []\n",
        "dictionary_prediction = {}\n",
        "\n",
        "with open(\"bigrams.txt\", errors='replace') as f:\n",
        "    for line in f:\n",
        "        bigrams.append(line.strip())\n",
        "\n",
        "\n",
        "for i in range(len(bigrams) - 1):\n",
        "    bigrams[i] = [int(x) if idx == 0 else x for idx, x in enumerate(bigrams[i].split(\"\\t\"))]\n",
        "    \n",
        "    if bigrams[i][0] > 1:\n",
        "        if bigrams[i][1]  in dictionary_prediction:\n",
        "            dictionary_prediction[bigrams[i][1]].add((bigrams[i][0], bigrams[i][2]))\n",
        "        else:\n",
        "            dictionary_prediction[bigrams[i][1]] = {(bigrams[i][0], bigrams[i][2])}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_next_word(bigram):\n",
        "    possibles = []\n",
        "    next_word_prediction = dictionary_prediction[bigram] if bigram in dictionary_prediction else dictionary_prediction[\"a\"]\n",
        "    for word in next_word_prediction:\n",
        "        possibles.append(word[1])\n",
        "    return possibles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open(\"coca_all_links.txt\", \"r\") as f:\n",
        "    all_links = f.read()\n",
        "\n",
        "dictionary = [word for word in word_tokenize(re.sub(r'[^\\w\\s]', '', all_links.lower()))]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fix_misspelled(all_links):\n",
        "    fixed_words = []\n",
        "    previous_word = \"a\"\n",
        "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', all_links.lower()))\n",
        "\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            candidates = predict_next_word(previous_word)\n",
        "            ranked_candidates = sorted(candidates, key=lambda word_a: nltk.edit_distance(word, word_a))\n",
        "            fix = ranked_candidates[0] if ranked_candidates else 0\n",
        "            fixed_words.append(fix if fix else word)\n",
        "        else:\n",
        "            fixed_words.append(word)\n",
        "        previous_word = word\n",
        "\n",
        "    return ' '.join(fixed_words)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oML-5sJwGRLE"
      },
      "source": [
        "## Justify your decisions\n",
        "\n",
        "Write down justificaitons for your implementation choices. For example, these choices could be:\n",
        "- Which ngram dataset to use\n",
        "- Which weights to assign for edit1, edit2 or absent words probabilities\n",
        "- Beam search parameters\n",
        "- etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Xb_twOmVsC6"
      },
      "source": [
        "## Explanation\n",
        "\n",
        "First of all, it is required to learn whether the word is misspelled or right written. To identify this I've downloaded a file containing correctly spelled words. The original text is a book from norvig which is a book written by Doyle. It's an old book, so it can somehow reflect to the language. But generally it should works quite well. \n",
        "\n",
        "I chose the bigrams, because of the high sensitivity of 5-grams due to pattern consisting of 5 words. \n",
        "\n",
        "To solve it, I worked with a word precedes to the misspelled word. I thought that the following word is redundant, so I decided not to take it into consideration. Usually the correct word is already in the candidates' list, if not so, it is most probably neither in predecessor nor successor. Due to this my solution became much easier and faster. \n",
        "\n",
        "I sorted all the candidates gained by bigrams to choose the most appropriate one. To calculate the weights I took the distance between the words. I used nltk.edit_distance (which uses Levenshtein edit distance)\n",
        "\n",
        "Sometimes situation can happen that no word is included in the birgram vocabulary, and exactly for this case I return candidates for the \"a\" due to it's being one of the most frequent words. And there are a lot of possible candidates for this word. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46rk65S4GRSe"
      },
      "source": [
        "## Evaluate on a test set\n",
        "\n",
        "Your task is to generate a test set and evaluate your work. You may vary the noise probability to generate different datasets with varying compexity. Compare your solution to the Norvig's corrector, and report the accuracies."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_words(text): \n",
        "    return re.findall(r'\\w+', text.lower())\n",
        "\n",
        "WORDS = Counter(find_words(all_links))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "def edits_one(word):\n",
        "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
        "    splits, deletes, transposes, replaces, inserts = [(word[:i], word[i:]) for i in range(len(word) + 1)], [], [], [], []\n",
        "\n",
        "    for left, right in splits:\n",
        "        if right:\n",
        "            deletes.append(left + right[1:])\n",
        "        if len(right) > 1:\n",
        "            transposes.append(left + right[1] + right[0] + right[2:])\n",
        "        for c in letters:\n",
        "            if right:\n",
        "                replaces.append(left + c + right[1:])\n",
        "            inserts.append(left + c + right)\n",
        "    \n",
        "    return set(deletes + transposes + replaces + inserts)\n",
        "\n",
        "def edits_two(word):\n",
        "    all_edits = []\n",
        "    for e1 in edits_one(word):\n",
        "        for e2 in edits_one(e1):\n",
        "            all_edits.append(e2)\n",
        "    return set(all_edits)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {},
      "outputs": [],
      "source": [
        "def find_candidates(word):\n",
        "    candidate_words = set()\n",
        "    if word in WORDS:\n",
        "        candidate_words.add(word)\n",
        "    \n",
        "    edits1_words = edits_one(word)\n",
        "    for edit in edits1_words:\n",
        "        if edit in WORDS:\n",
        "            candidate_words.add(edit)\n",
        "    \n",
        "    edits2_words = edits_two(word)\n",
        "    for edit in edits2_words:\n",
        "        if edit in WORDS:\n",
        "            candidate_words.add(edit)\n",
        "\n",
        "    if not candidate_words:\n",
        "        candidate_words.add(word)\n",
        "    \n",
        "    return candidate_words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {},
      "outputs": [],
      "source": [
        "def calculate_probability(word, N=sum(WORDS.values())): \n",
        "    return WORDS[word] / N\n",
        "\n",
        "def fixing(word): \n",
        "    return max(find_candidates(word), key=calculate_probability)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {},
      "outputs": [],
      "source": [
        "def fix_misspelled_Norvig(text):\n",
        "    fixed_words = []\n",
        "    words = word_tokenize(re.sub(r'[^\\w\\s]', '', text.lower()))\n",
        "    for word in words:\n",
        "        if word not in dictionary:\n",
        "            fixed = fixing(word)\n",
        "            fixed_words.append(fixed if fixing else word)\n",
        "        else:\n",
        "            fixed_words.append(word)\n",
        "    return ' '.join(fixed_words)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [],
      "source": [
        "def add_noise(text, frequency):\n",
        "    noisy_text = \"\"\n",
        "    text = text.lower()\n",
        "    for word in text:\n",
        "        noisy_text += rd.choice(string.ascii_letters).lower() if (rd.randint(0, frequency) == 1 and word.isalpha()) else word\n",
        "    return noisy_text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_score(text1, text2):\n",
        "    score = 0\n",
        "    text1, text2 = text1.split(), text2.split()\n",
        "    length = len(text1)\n",
        "    for x, y in zip(text1, text2):\n",
        "        score += 1 if x == y else 0\n",
        "    return(score / length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {},
      "outputs": [],
      "source": [
        "sample_text = '''\n",
        "I once used an activity-centered control, setting it to present my photographs\n",
        "to the audience. All worked well until I was asked a question. I paused to answer\n",
        "it, but wanted to raise the room lights so I could see the audience. No, the\n",
        "activity of giving a talk with visually presented images meant that room lights\n",
        "were fixed at a dim setting. When I tried to increase the light intensity, this took\n",
        "me out of “giving a talk” activity, so I did get the light to where I wanted it, but\n",
        "the projection screen also went up into the ceiling and the projector was turned\n",
        "off. The difficulty with activity-based controllers is handling the exceptional\n",
        "cases, the ones not thought about during design.\n",
        "Activity-centered controls are the proper way to go, if the activities are\n",
        "carefully selected to match actual requirements. But even in these cases, manual\n",
        "controls will still be required because there will always be some new, unexpected\n",
        "demand that requires idiosyncratic settings. As my example demonstrates,\n",
        "invoking the manual settings should not cause the current activity to be canceled.'''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1mEasy test \u001b[0m\n",
            "My score is: 0.7540983606557377\n",
            "Norvig's solution score: 0.5901639344262295\n",
            "\n",
            "\n",
            "\u001b[1mMedium test \u001b[0m\n",
            "My score is: 0.7595628415300546\n",
            "Norvig's solution score: 0.5792349726775956\n",
            "\n",
            "\n",
            "\u001b[1mHard test \u001b[0m\n",
            "My score is: 0.7377049180327869\n",
            "Norvig's solution score: 0.5628415300546448\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "text_misspelled = add_noise(sample_text, 80)\n",
        "my_corrected_text = fix_misspelled(text_misspelled)\n",
        "norvig = fix_misspelled_Norvig(text_misspelled)\n",
        "my_score = get_score(sample_text, my_corrected_text)\n",
        "norvig_score = get_score(sample_text, norvig)\n",
        "\n",
        "print(f\"\\033[1mEasy test \\033[0m\")\n",
        "print(f\"My score is: {my_score}\")\n",
        "print(f\"Norvig's solution score: {norvig_score}\")\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "text_misspelled = add_noise(sample_text, 60)\n",
        "my_corrected_text = fix_misspelled(text_misspelled)\n",
        "norvig = fix_misspelled_Norvig(text_misspelled)\n",
        "my_score = get_score(sample_text, my_corrected_text)\n",
        "norvig_score = get_score(sample_text, norvig)\n",
        "\n",
        "print(f\"\\033[1mMedium test \\033[0m\")\n",
        "print(f\"My score is: {my_score}\")\n",
        "print(f\"Norvig's solution score: {norvig_score}\")\n",
        "print('\\n')\n",
        "\n",
        "\n",
        "text_misspelled = add_noise(sample_text, 40)\n",
        "my_correction = fix_misspelled(text_misspelled)\n",
        "norvig = fix_misspelled_Norvig(text_misspelled)\n",
        "my_score = get_score(sample_text, my_correction)\n",
        "norvig_score = get_score(sample_text, norvig)\n",
        "\n",
        "print(f\"\\033[1mHard test \\033[0m\")\n",
        "print(f\"My score is: {my_score}\")\n",
        "print(f\"Norvig's solution score: {norvig_score}\")\n",
        "print('\\n')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
